{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-25 19:52:58.825576: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-25 19:52:58.970638: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-03-25 19:52:58.970655: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-03-25 19:52:59.687268: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-25 19:52:59.687394: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-25 19:52:59.687401: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "# data processing tools\n",
    "import string, os \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "\n",
    "import random \n",
    "\n",
    "# keras module for building LSTM \n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(42)\n",
    "import tensorflow.keras.utils as ku \n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "\n",
    "# surpress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning) # Ignore warnings from libraries. \n",
    "\n",
    "#import sys\n",
    "#sys.path.append(\"..\")\n",
    "#import utils.requirement_functions as rf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ross functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(txt): # return vocab if it is not part of string.punctuation \n",
    "    # string.punctuation is a python model. ( a list of all string characters that er punctuations /%&Â¤#\";:_-.,*\")\n",
    "    txt = \"\".join(v for v in txt if v not in string.punctuation).lower() # Making lower case \n",
    "    txt = txt.encode(\"utf8\").decode(\"ascii\",'ignore') # encoding utf8\n",
    "    return txt \n",
    "\n",
    "def get_sequence_of_tokens(tokenizer, corpus):\n",
    "    ## convert data to sequence of tokens \n",
    "    input_sequences = []\n",
    "    for line in corpus: # every head \n",
    "        token_list = tokenizer.texts_to_sequences([line])[0] # list of tokens \n",
    "        for i in range(1, len(token_list)): # order dem sequentialy\n",
    "            n_gram_sequence = token_list[:i+1]\n",
    "            input_sequences.append(n_gram_sequence)\n",
    "    return input_sequences\n",
    "\n",
    "def generate_padded_sequences(input_sequences):\n",
    "    # get the length of the longest sequence\n",
    "    max_sequence_len = max([len(x) for x in input_sequences])\n",
    "    # make every sequence the length of the longest on\n",
    "    input_sequences = np.array(pad_sequences(input_sequences, \n",
    "                                            maxlen=max_sequence_len, \n",
    "                                            padding='pre'))\n",
    "\n",
    "    predictors, label = input_sequences[:,:-1],input_sequences[:,-1]\n",
    "    label = ku.to_categorical(label, \n",
    "                            num_classes=total_words)\n",
    "    return predictors, label, max_sequence_len\n",
    "\n",
    "def create_model(max_sequence_len, total_words): # model initilisation \n",
    "    input_len = max_sequence_len - 1\n",
    "    model = Sequential() # sequential model\n",
    "    # Add Input Embedding Layer\n",
    "    model.add(Embedding(total_words, #\n",
    "                        10, \n",
    "                        input_length=input_len))\n",
    "    # Add Hidden Layer 1 - LSTM Layer\n",
    "    model.add(LSTM(100)) # long short term memory\n",
    "    model.add(Dropout(0.1)) # drop out layer, during training everytime you make an iteration 10% of the weights should be removed. \n",
    "    # so every iteration is only 90 %. Making things a bit more diffiuclt for the model \n",
    "    # Add Output Layer\n",
    "    model.add(Dense(total_words, \n",
    "                    activation='softmax')) # Softmax prediction.\n",
    "    model.compile(loss='categorical_crossentropy', \n",
    "                    optimizer='adam')\n",
    "    \n",
    "    return model\n",
    "\n",
    "def generate_text(seed_text, next_words, model, max_sequence_len): # seed_text = prompt.\n",
    "    for _ in range(next_words): # for how ever many in next_word.\n",
    "        token_list = tokenizer.texts_to_sequences([seed_text])[0] # get vocab \n",
    "        token_list = pad_sequences([token_list],  # pad it (zeros)\n",
    "                                    maxlen=max_sequence_len-1, \n",
    "                                    padding='pre')\n",
    "        predicted = np.argmax(model.predict(token_list), # predict the next words with higest score.\n",
    "                                            axis=1)\n",
    "        \n",
    "        output_word = \"\"\n",
    "        for word,index in tokenizer.word_index.items(): # appending words together. \n",
    "            if index == predicted:\n",
    "                output_word = word\n",
    "                break\n",
    "        seed_text += \" \"+output_word\n",
    "    return seed_text.title()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading data \n",
    "def filepath ():\n",
    "    print(\"Loading data\")\n",
    "    data_dir = os.path.join(\"..\",\"data\", \"news_data\")\n",
    "    return data_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Appending columns\n",
    "def creating_list(file_path): \n",
    "    all_comments = []\n",
    "    for filename in os.listdir(file_path):\n",
    "        if 'Comments' in filename:\n",
    "            comment_df = pd.read_csv(data_dir + \"/\" + filename) # joining data_dir / filename. ( Creating dataframe)\n",
    "            all_comments.extend(list(comment_df[\"commentBody\"].values)) # Creating a list of only comments. \n",
    "    print(\"Amount of comments: \" + len(all_comments))\n",
    "    return all_comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MAking a new list with 1000 random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_sampling(comments_list):\n",
    "    thousand_comments = random.sample(comments_list, 1000)\n",
    "    print(\"Sample size: \" + len(thousand_comments))\n",
    "    return thousand_comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning_comments(sample_list):\n",
    "    print(\"Cleaning text\")\n",
    "    corpus = [clean_text(x) for x in sample_list]\n",
    "    \n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenization(clean_data):\n",
    "    print(\"Tokenizing\")\n",
    "    tokenizer = Tokenizer()\n",
    "    ## tokenization\n",
    "    tokenizer.fit_on_texts(clean_data) # tokenizing the text, and gives every word an index. Creating a vocab.\n",
    "    total_words = len(tokenizer.word_index) + 1 # how many total words are there. The reason for + 1 is to account for  = out of vocabulary token. if the tensorflow does not know the word. <unk> unknown word.\n",
    "    \n",
    "    return tokenizer, total_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[4588, 2139],\n",
       " [4588, 2139, 718],\n",
       " [4588, 2139, 718, 91],\n",
       " [4588, 2139, 718, 91, 76],\n",
       " [4588, 2139, 718, 91, 76, 4589],\n",
       " [4588, 2139, 718, 91, 76, 4589, 37],\n",
       " [4588, 2139, 718, 91, 76, 4589, 37, 509],\n",
       " [4588, 2139, 718, 91, 76, 4589, 37, 509, 809],\n",
       " [4588, 2139, 718, 91, 76, 4589, 37, 509, 809, 3],\n",
       " [4588, 2139, 718, 91, 76, 4589, 37, 509, 809, 3, 581]]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def input_sequence_function(tokenizer, clean_data):\n",
    "    print(\"Input sequence\")\n",
    "    inp_sequences = get_sequence_of_tokens(tokenizer, clean_data)\n",
    "    # Each document has multiple rows. 1-2, 1-2-3, 1-2-3-4 words (n-grams)\n",
    "    # Teaching the model to account to longer distances. \n",
    "    return inp_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padded_sequences(input_sequence):\n",
    "    print(\"Padding sequences\")\n",
    "    predictors, label, max_sequence_len = generate_padded_sequences(input_sequence) \n",
    "    # All inputs need to be same lenght. \n",
    "    # adding zeros to the start of shorted sequences \n",
    "    # predictors = input vectors \n",
    "    # labels = words \n",
    "    print(\"Max sequence length: \" + max_sequence_len)\n",
    "    return predictors, label, max_sequence_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(sequnece_length, total_words):\n",
    "    print(\"Creating model\")\n",
    "    model = create_model(sequnece_length, total_words)\n",
    "    print(model.summary())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "536/536 [==============================] - 237s 443ms/step - loss: 7.1720\n",
      "Epoch 2/10\n",
      "536/536 [==============================] - 237s 442ms/step - loss: 6.9859\n",
      "Epoch 3/10\n",
      "536/536 [==============================] - 237s 442ms/step - loss: 6.8728\n",
      "Epoch 4/10\n",
      "536/536 [==============================] - 237s 442ms/step - loss: 6.7628\n",
      "Epoch 5/10\n",
      "536/536 [==============================] - 237s 442ms/step - loss: 6.6631\n",
      "Epoch 6/10\n",
      "536/536 [==============================] - 237s 443ms/step - loss: 6.5566\n",
      "Epoch 7/10\n",
      "536/536 [==============================] - 237s 442ms/step - loss: 6.4378\n",
      "Epoch 8/10\n",
      "536/536 [==============================] - 238s 445ms/step - loss: 6.3110\n",
      "Epoch 9/10\n",
      "536/536 [==============================] - 239s 446ms/step - loss: 6.1812\n",
      "Epoch 10/10\n",
      "536/536 [==============================] - 239s 446ms/step - loss: 6.0513\n"
     ]
    }
   ],
   "source": [
    "def training_model(model):\n",
    "    print(\"Training model\")\n",
    "    history = model.fit(predictors, \n",
    "                        label, \n",
    "                        epochs=1, # prev. 100\n",
    "                        batch_size=128, # Updates weights after 128 \n",
    "                        verbose=1)\n",
    "return history\n",
    "\n",
    "# In notebooks, a models history is saved. So if the model has run one time with 100 epoch and you start it again it will run for 200 intotal.\n",
    "# You either need to create the model again ( Above chunck) or use tensor flow functiion clear history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "Hello Is The Same Of The Us Of The Us Of\n"
     ]
    }
   ],
   "source": [
    "print (generate_text(\"Hello\", 10, model, max_sequence_len)) # word you want, words to come after, model, make the sequence 24 in total."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_function():\n",
    "    data_dir = filepath()\n",
    "    all_comments = creating_list(data_dir)\n",
    "    thousand_comments = data_sampling(allcomments)\n",
    "    corpus = cleaning_comments(clean_text, thousand_comments)\n",
    "    tokenizer, total_words = tokenization(corpus)\n",
    "    inp_sequences = input_sequence_function(tokenizer, corpus)\n",
    "    predictors, label, max_sequence_len = padded_sequences(inp_sequences)\n",
    "    model = create_model(max_sequence_len, total_words)\n",
    "    history = training_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'data_dir' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m main_function()\n",
      "Cell \u001b[0;32mIn[13], line 3\u001b[0m, in \u001b[0;36mmain_function\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmain_function\u001b[39m():\n\u001b[1;32m      2\u001b[0m     data_dir \u001b[39m=\u001b[39m filepath()\n\u001b[0;32m----> 3\u001b[0m     all_comments \u001b[39m=\u001b[39m creating_list(data_dir)\n\u001b[1;32m      4\u001b[0m     thousand_comments \u001b[39m=\u001b[39m data_sampling(allcomments)\n\u001b[1;32m      5\u001b[0m     corpus \u001b[39m=\u001b[39m cleaning_comments(clean_text, thousand_comments)\n",
      "Cell \u001b[0;32mIn[5], line 6\u001b[0m, in \u001b[0;36mcreating_list\u001b[0;34m(file_path)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[39mfor\u001b[39;00m filename \u001b[39min\u001b[39;00m os\u001b[39m.\u001b[39mlistdir(file_path):\n\u001b[1;32m      5\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mComments\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m filename:\n\u001b[0;32m----> 6\u001b[0m         comment_df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(data_dir \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m/\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m filename) \u001b[39m# joining data_dir / filename. ( Creating dataframe)\u001b[39;00m\n\u001b[1;32m      7\u001b[0m         all_comments\u001b[39m.\u001b[39mextend(\u001b[39mlist\u001b[39m(comment_df[\u001b[39m\"\u001b[39m\u001b[39mcommentBody\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mvalues)) \u001b[39m# Creating a list of only comments. \u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mAmount of comments: \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mlen\u001b[39m(all_comments))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data_dir' is not defined"
     ]
    }
   ],
   "source": [
    "main_function()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
