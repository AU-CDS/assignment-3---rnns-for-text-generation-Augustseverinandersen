{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data processing tools\n",
    "import string, os \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "\n",
    "import random \n",
    "\n",
    "# keras module for building LSTM \n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(42)\n",
    "import tensorflow.keras.utils as ku \n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "\n",
    "# surpress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning) # Ignore warnings from libraries. \n",
    "\n",
    "#import sys\n",
    "#sys.path.append(\"..\")\n",
    "#import utils.requirement_functions as rf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ross functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(txt): # return vocab if it is not part of string.punctuation \n",
    "    # string.punctuation is a python model. ( a list of all string characters that er punctuations /%&Â¤#\";:_-.,*\")\n",
    "    txt = \"\".join(v for v in txt if v not in string.punctuation).lower() # Making lower case \n",
    "    txt = txt.encode(\"utf8\").decode(\"ascii\",'ignore') # encoding utf8\n",
    "    return txt \n",
    "\n",
    "def get_sequence_of_tokens(tokenizer, corpus):\n",
    "    ## convert data to sequence of tokens \n",
    "    input_sequences = []\n",
    "    for line in corpus: # every head \n",
    "        token_list = tokenizer.texts_to_sequences([line])[0] # list of tokens \n",
    "        for i in range(1, len(token_list)): # order dem sequentialy\n",
    "            n_gram_sequence = token_list[:i+1]\n",
    "            input_sequences.append(n_gram_sequence)\n",
    "    return input_sequences\n",
    "\n",
    "def generate_padded_sequences(input_sequences):\n",
    "    # get the length of the longest sequence\n",
    "    max_sequence_len = max([len(x) for x in input_sequences])\n",
    "    # make every sequence the length of the longest on\n",
    "    input_sequences = np.array(pad_sequences(input_sequences, \n",
    "                                            maxlen=max_sequence_len, \n",
    "                                            padding='pre'))\n",
    "\n",
    "    predictors, label = input_sequences[:,:-1],input_sequences[:,-1]\n",
    "    label = ku.to_categorical(label, \n",
    "                            num_classes=total_words)\n",
    "    return predictors, label, max_sequence_len\n",
    "\n",
    "def create_model(max_sequence_len, total_words): # model initilisation \n",
    "    input_len = max_sequence_len - 1\n",
    "    model = Sequential() # sequential model\n",
    "    # Add Input Embedding Layer\n",
    "    model.add(Embedding(total_words, #\n",
    "                        10, \n",
    "                        input_length=input_len))\n",
    "    # Add Hidden Layer 1 - LSTM Layer\n",
    "    model.add(LSTM(100)) # long short term memory\n",
    "    model.add(Dropout(0.1)) # drop out layer, during training everytime you make an iteration 10% of the weights should be removed. \n",
    "    # so every iteration is only 90 %. Making things a bit more diffiuclt for the model \n",
    "    # Add Output Layer\n",
    "    model.add(Dense(total_words, \n",
    "                    activation='softmax')) # Softmax prediction.\n",
    "    model.compile(loss='categorical_crossentropy', \n",
    "                    optimizer='adam')\n",
    "    \n",
    "    return model\n",
    "\n",
    "def generate_text(seed_text, next_words, model, max_sequence_len): # seed_text = prompt.\n",
    "    for _ in range(next_words): # for how ever many in next_word.\n",
    "        token_list = tokenizer.texts_to_sequences([seed_text])[0] # get vocab \n",
    "        token_list = pad_sequences([token_list],  # pad it (zeros)\n",
    "                                    maxlen=max_sequence_len-1, \n",
    "                                    padding='pre')\n",
    "        predicted = np.argmax(model.predict(token_list), # predict the next words with higest score.\n",
    "                                            axis=1)\n",
    "        \n",
    "        output_word = \"\"\n",
    "        for word,index in tokenizer.word_index.items(): # appending words together. \n",
    "            if index == predicted:\n",
    "                output_word = word\n",
    "                break\n",
    "        seed_text += \" \"+output_word\n",
    "    return seed_text.title()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data\n"
     ]
    }
   ],
   "source": [
    "# Loading data \n",
    "print(\"Loading data\")\n",
    "data_dir = os.path.join(\"..\",\"data\", \"news_data\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Appending columns \n",
    "all_comments = []\n",
    "for filename in os.listdir(data_dir):\n",
    "    if 'Comments' in filename:\n",
    "        comment_df = pd.read_csv(data_dir + \"/\" + filename) # joining data_dir / filename. ( Creating dataframe)\n",
    "        all_comments.extend(list(comment_df[\"commentBody\"].values)) # Creating a list of only comments. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2176364"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_comments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MAking a new list with 1000 random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "thousand_comments = random.sample(all_comments, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(thousand_comments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hi iq explain then why wyoming has 2 senators and california has 2 senators brbrif you were a high iq foreign country say which state senators would be easiest to buy with your money and which presidential candidate would you supportbrbras an exercise for the highest iq cabinet ever study the campaign contributions to the low population states visavis the high population states look at the  per vote by state by foreign interest by foreign aid ok smarty pants',\n",
       " 'memo to the trump administratiionbrbrif you cant learn to live with reality reality will come to live with you',\n",
       " 'neither you nor i have much power to do anything about this manchild but congressional republicans do when will they put country before party and stop protecting the president',\n",
       " 'they tell us if we see something say something  but they didnt say what would happen afterwards apparently nothing  this is shatteringly sad',\n",
       " 'actually no  their competition in china actually beat uber at this game and forcing uber to sell out to them',\n",
       " 'this piece leaves out two major factorsbrbr1 many states such as my home state of california have increasing requirements that utilities use renewable sources  so the potential alternatives to solar are largely limited to wind not fossil fuels this should not hurt solar much when it is located in favorable locations  if places where solar has a slight advantage over wind in the pretariff market wind energy will be used instead  brbr2 solar electricity receives very large federal subsidies through tax policy  so in a sense these tariffs have the result of having larger net subsidies when the panels are made by us workers as oppose to imports  i think most tax payers would favor that policy over subsiding imports as much as domestic production',\n",
       " 'sure changes in antitrumpttump splits in the electorate are bigger for sex than for race  sexual splits starr out relatively small like 5545 and seldom shaper than 6040 initialbrracial ones tend to much larger skewed for black white differences like 2080 and beyond the  outer limits of contrast for hispanicswhites skewed antitrump differences one involving blacks are very unlikely to chance in an anti trump direction they have so little room to change sex differencee are free of floor  and ceiling effects free to move around',\n",
       " 'and they tax income at a 15 ratebrbrsweden is an actual first world country  we havent reached that status  when we compare swedens medical care and education system with the us system you right wingers tell us we shouldnt be comparing  yeah well democratic socialism hurts the ears of republicans even though it is clearly a better form of government than oppressive plutocracy that we have herebrbrso you tell us not to compare what is best about sweden and only look at their tax policy where you want us to look  more republican misinformation  one might think its a conspiracy of misinformation or one might be able to actually prove  that it is intentional misinformation right',\n",
       " 'finally not only do others dump metals here at rediulously low prices with their governments assistance many times they are not of the same quality produced here be it a bolt or an ibar they are putting many lives in danger',\n",
       " 'i understand that mexico has an open offer to mexican born daca recipients to come back  no questions asked  they re a potential asset to that countrys development  many if not most live in texas and california which are little different from northern mexico']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = [clean_text(x) for x in thousand_comments]\n",
    "corpus[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10957"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = Tokenizer()\n",
    "## tokenization\n",
    "tokenizer.fit_on_texts(corpus) # tokenizing the text, and gives every word an index. Creating a vocab.\n",
    "total_words = len(tokenizer.word_index) + 1 # how many total words are there. The reason for + 1 is to account for  = out of vocabulary token. if the tensorflow does not know the word. <unk> unknown word.\n",
    "total_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[4588, 2139],\n",
       " [4588, 2139, 718],\n",
       " [4588, 2139, 718, 91],\n",
       " [4588, 2139, 718, 91, 76],\n",
       " [4588, 2139, 718, 91, 76, 4589],\n",
       " [4588, 2139, 718, 91, 76, 4589, 37],\n",
       " [4588, 2139, 718, 91, 76, 4589, 37, 509],\n",
       " [4588, 2139, 718, 91, 76, 4589, 37, 509, 809],\n",
       " [4588, 2139, 718, 91, 76, 4589, 37, 509, 809, 3],\n",
       " [4588, 2139, 718, 91, 76, 4589, 37, 509, 809, 3, 581]]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp_sequences = get_sequence_of_tokens(tokenizer, corpus)\n",
    "inp_sequences[:10] # Each document has multiple rows. 1-2, 1-2-3, 1-2-3-4 words (n-grams)\n",
    "# Teaching the model to account to longer distances. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictors, label, max_sequence_len = generate_padded_sequences(inp_sequences) \n",
    "# All inputs need to be same lenght. \n",
    "# adding zeros to the start of shorted sequences \n",
    "# predictors = input vectors \n",
    "# labels = words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "264"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_sequence_len # 264"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-24 16:42:37.168958: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2023-03-24 16:42:37.169022: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-03-24 16:42:37.169071: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (j-688397-job-0): /proc/driver/nvidia/version does not exist\n",
      "2023-03-24 16:42:37.170068: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 263, 10)           109570    \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 100)               44400     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 100)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10957)             1106657   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,260,627\n",
      "Trainable params: 1,260,627\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = create_model(max_sequence_len, total_words)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "536/536 [==============================] - 237s 443ms/step - loss: 7.1720\n",
      "Epoch 2/10\n",
      "536/536 [==============================] - 237s 442ms/step - loss: 6.9859\n",
      "Epoch 3/10\n",
      "536/536 [==============================] - 237s 442ms/step - loss: 6.8728\n",
      "Epoch 4/10\n",
      "536/536 [==============================] - 237s 442ms/step - loss: 6.7628\n",
      "Epoch 5/10\n",
      "536/536 [==============================] - 237s 442ms/step - loss: 6.6631\n",
      "Epoch 6/10\n",
      "536/536 [==============================] - 237s 443ms/step - loss: 6.5566\n",
      "Epoch 7/10\n",
      "536/536 [==============================] - 237s 442ms/step - loss: 6.4378\n",
      "Epoch 8/10\n",
      "536/536 [==============================] - 238s 445ms/step - loss: 6.3110\n",
      "Epoch 9/10\n",
      "536/536 [==============================] - 239s 446ms/step - loss: 6.1812\n",
      "Epoch 10/10\n",
      "536/536 [==============================] - 239s 446ms/step - loss: 6.0513\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(predictors, \n",
    "                    label, \n",
    "                    epochs=10, # prev. 100\n",
    "                    batch_size=128, # prev. 128 # Updates weights after 128 \n",
    "                    verbose=1)\n",
    "\n",
    "# In notebooks, a models history is saved. So if the model has run one time with 100 epoch and you start it again it will run for 200 intotal.\n",
    "# You either need to create the model again ( Above chunck) or use tensor flow functiion clear history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "Hello Is The Same Of The Us Of The Us Of\n"
     ]
    }
   ],
   "source": [
    "print (generate_text(\"Hello\", 10, model, max_sequence_len)) # word you want, words to come after, model, make the sequence 24 in total."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
